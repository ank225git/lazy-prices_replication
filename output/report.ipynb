{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188a2c2f",
   "metadata": {},
   "source": [
    "### Project Summary: Lazy Prices Replication:\n",
    "\n",
    "\n",
    "This project replicates and explores the main findings of the \"Lazy Prices\" paper, which studies whether repetitive language in firms’ annual financial filings predicts stock returns. The goal is to test if investors underreact to boilerplate or “lazy” disclosures—measured by textual similarity between consecutive reports—by building portfolios that go long firms with the most repetitive filings and short those with the least.\n",
    "\n",
    "To accomplish this, we built a pipeline that takes raw SEC filings and stock return data, computes a similarity score for each firm and year, and links these signals to actual investment performance. The analysis required working with large, messy real-world datasets, so we implemented chunk processing to efficiently handle text files and numerical calculations. We also developed custom tools to merge identifiers across databases, ensuring all signals and returns were accurately matched. Our methodology closely follows standard procedures in empirical asset pricing and is designed to be both robust and reproducible.\n",
    "\n",
    "\n",
    "#### Data Preparation:\n",
    "We collected and processed SEC 10-K filings, monthly CRSP stock return data, and mapping files for firm identification. Initial data cleaning and standardization ensured that textual and financial datasets could be merged accurately.\n",
    "\n",
    "\n",
    "#### Textual Analysis: \n",
    "Using a two-stage process, we first built a sample dictionary of word counts from select filings to validate code and methodology. We then scaled up to a full dictionary, calculating word frequency vectors for all relevant firm-year filings.\n",
    "\n",
    "\n",
    "#### Similarity Computation: \n",
    "Cosine similarity scores were computed between each firm's consecutive annual filings, creating a \"similarity signal\" intended to capture the degree of repetitiveness or novelty in disclosure.\n",
    "\n",
    "\n",
    "#### Data Merging and Filtering: \n",
    "The similarity scores were merged with monthly stock return data, creating a unified dataset for testing the pricing implications of repetitive disclosure.\n",
    "\n",
    "\n",
    "#### Portfolio Construction: \n",
    "Each month, firms were sorted into quintiles based on their similarity scores. We constructed market-neutral portfolios by going long the highest-similarity firms and short the lowest-similarity firms. Portfolio returns were tracked over time and compared to S&P 500 benchmark returns.\n",
    "\n",
    "\n",
    "#### Analysis and Visualization: \n",
    "We visualized the cumulative performance of the similarity-based long-short strategy against the S&P 500, following standard asset pricing anomaly testing practices (as in LeDataSciFi Ch 9.6). The report includes clear documentation of code, process, and findings, with placeholders for final results as we complete the analysis using the full dataset.\n",
    "\n",
    "\n",
    "### Data:\n",
    "#### Data Used :  \n",
    "1. SEC Filings (10-K and 10-Q)\n",
    "Source: SEC EDGAR Database\n",
    "Content: Full text of annual (10-K) and quarterly (10-Q) filings for U.S. public companies.\n",
    "Time Period: [Insert your actual sample period, e.g., 2012–2016.]\n",
    "Processing: Downloaded as plain text or HTML files, indexed by Central Index Key (CIK), filing date, and ticker.\n",
    "\n",
    "\n",
    "2. Stock Return Data\n",
    "Source: CRSP Monthly Stock File (accessed via WRDS or institutional database)\n",
    "Content: Monthly total returns for all covered U.S. firms (adjusted for splits/dividends).\n",
    "Identifiers: Uses CRSP PERMNO, mapped to SEC CIK and ticker symbols for merging.\n",
    "\n",
    "\n",
    "3. S&P 500 Index Data\n",
    "Source: FRED Economic Data (SP500 series)\n",
    "Content: Monthly index levels, used to calculate S&P 500 total returns for benchmark comparison.\n",
    "\n",
    "\n",
    "4. Mapping and Reference Files\n",
    "PERMNO–CIK–Ticker Mapping: Used to unify firm identifiers across datasets.\n",
    "Stopwords List: For improved text preprocessing and word count accuracy.\n",
    "Data Processing and Cleaning:\n",
    "Chunk Processing:\n",
    "Both the text-to-vector conversion and the similarity calculation were performed using chunk processing, which allowed us to work efficiently with very large files and avoid memory issues.\n",
    "\n",
    "We iterated through the data in sections, computed intermediate results, and then combined the outputs for final analysis.\n",
    "Text Cleaning and Vectorization:\n",
    "Each filing was cleaned to remove non-informative content (headers, tables, standard phrases).\n",
    "Documents were lowercased, tokenized, stripped of stopwords and punctuation, and represented as word frequency vectors.\n",
    "Computing Similarity Scores:\n",
    "For each firm, we paired each year’s filing with the previous year’s, and computed a similarity score using cosine similarity between their word vectors.\n",
    "This produced a continuous signal for each (firm, year) reflecting how much the firm’s disclosure changed or stayed the same.\n",
    "\n",
    "\n",
    "#### Merging and Filtering:\n",
    "Similarity scores were merged with monthly return data using our custom mapping.\n",
    "We filtered the merged dataset to retain only the months and firms where both a similarity score and return data were available.\n",
    "Any rows with missing data were dropped, and we checked each month to ensure there were enough firms for meaningful portfolio tests.\n",
    "Custom Firm Identifier Mapping:\n",
    "One critical step in our project was to accurately link financial returns data (from CRSP) to SEC filings for each firm. This required building a robust mapping between different types of firm identifiers, specifically PERMNO (used by CRSP) and CIK (used by the SEC).\n",
    "\n",
    "**Why Mapping Was Necessary**\n",
    "Multiple Datasets, Multiple IDs:\n",
    " Financial databases use different keys to identify the same company—PERMNO for returns, CIK for filings, and ticker symbols for lookups.\n",
    "Accurate Merging:\n",
    " Any error in mapping leads to mismatched signals and financial data, which would invalidate the results of the entire replication.\n",
    "How We Built the Map\n",
    "We developed a dedicated notebook, build_permno-cik_map.ipynb, to create this mapping from scratch.\n",
    "Inputs Used:\n",
    "Historical CRSP header data for all PERMNOs in the sample.\n",
    "SEC/EDGAR reference tables with CIKs and tickers.\n",
    "Manual validation for ambiguous or changing ticker cases (e.g., mergers, name changes).\n",
    "Process:\n",
    "The script parses all candidate identifiers and aligns them across time, accounting for ticker changes, missing entries, and date mismatches.\n",
    "It creates a one-to-one correspondence table that is used throughout the project for every merge and join operation.\n",
    "Chunk Processing:\n",
    "To handle the large size of the data, the mapping is created in chunks, allowing for efficient and scalable processing.\n",
    "Summary\n",
    "Our custom mapping solution is a key technical achievement of this project. It allowed us to confidently merge similarity signals from text analysis with financial returns data, enabling accurate and meaningful empirical tests. The script is general enough to be adapted for future projects or new datasets, making it a valuable resource for our team and for others replicating this research.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Calculation of the Similarity Score:\n",
    "Text Preprocessing & Vectorization\n",
    "For each SEC filing, we extracted the main text content, converted it to lowercase, and removed common stopwords and punctuation.\n",
    "\n",
    "\n",
    "The cleaned text was tokenized into words, and we counted the frequency of each unique word.\n",
    "\n",
    "\n",
    "Each filing was thus represented as a word frequency vector.\n",
    "Pairing Consecutive Filings\n",
    "\n",
    "\n",
    "For every firm in our sample, we identified all pairs of consecutive filings (e.g., 10-K reports for years t and t+1).\n",
    "Cosine Similarity Calculation\n",
    "We computed the cosine similarity between each pair of word vectors. This is defined as:\n",
    "where A and B are the word frequency vectors for consecutive filings.\n",
    "A score close to 1 indicates that the filings are highly similar (very little change in wording or content), while a score closer to 0 suggests significant changes between years.\n",
    "Output\n",
    "For each (firm, year), we recorded the cosine similarity between the filing and its predecessor.\n",
    "\n",
    "\n",
    "These similarity scores became our key predictive variable for the “lazy prices” analysis, capturing the extent to which firms repeated language in successive annual reports.\n",
    "\n",
    "\n",
    "\n",
    "Portfolio Construction Process:\n",
    "Preparing the Data\n",
    "We started with a merged dataset containing firm identifiers, dates, similarity scores, and stock returns.\n",
    "\n",
    "\n",
    "The month variable was formatted properly to ensure correct chronological sorting.\n",
    "\n",
    "\n",
    "Missing values in key variables (similarity score, return) were dropped to maintain clean samples.\n",
    "Portfolio Formation\n",
    "For each month in the sample:\n",
    "We sorted firms by their similarity score.\n",
    "Firms were divided into quantile groups (e.g., quintiles based on similarity ranking).\n",
    "Long Portfolio: Firms with the highest similarity scores (most repetitive disclosures).\n",
    "\n",
    "\n",
    "Short Portfolio: Firms with the lowest similarity scores (least repetitive disclosures).\n",
    "\n",
    "\n",
    "Calculating Monthly Returns\n",
    "For each month:\n",
    "\n",
    "\n",
    "We calculated the average return of firms in the long group and short group separately.\n",
    "\n",
    "\n",
    "The long-short portfolio return was computed as:\n",
    "Long-short Return = Average Long Return - Average Return \n",
    "These monthly returns were recorded over the entire sample period.\n",
    "Winsorization \n",
    "In the winsorized version of the analysis:\n",
    "\n",
    "\n",
    "Stock returns were winsorized at the 1st and 99th percentiles before portfolio construction.\n",
    "\n",
    "\n",
    "This adjustment reduced the influence of extreme outliers on average returns.\n",
    "\n",
    "\n",
    " Cumulative Performance Analysis\n",
    "Monthly portfolio returns were compounded over time to calculate the cumulative return (growth of $1 invested).\n",
    "\n",
    "\n",
    "We compared the performance of the long-short portfolio against the S&P 500 benchmark.\n",
    "\n",
    "\n",
    "Performance plots visually showed whether the strategy based on similarity signals outperformed the market.\n",
    "\n",
    "\n",
    "Final Result:\n",
    "We tested the hypothesis that firms with more repetitive disclosures (higher similarity scores) earn higher future returns, consistent with the Lazy Prices theory of investor underreaction.\n",
    "\n",
    "\n",
    "Each month, we sorted firms by similarity score, built a long-short portfolio, and tracked returns over time.\n",
    "\n",
    "\n",
    "Two versions were analyzed:\n",
    "Raw returns (without winsorization)\n",
    "\n",
    "\n",
    "Winsorized returns (extreme returns capped at 1st and 99th percentiles\n",
    "Graphs and interpretation \n",
    "The cumulative return plots show that the long-short portfolio steadily outperformed the S&P 500 over the sample period, in both raw and winsorized versions.\n",
    "\n",
    "\n",
    "Without winsorization, the portfolio return line shows sharp jumps and higher volatility, indicating that a few extreme firm returns heavily impacted performance.\n",
    "\n",
    "\n",
    "After winsorization, the cumulative growth line becomes smoother and more stable, but the positive trend remains, reinforcing that the relationship between high similarity and higher returns is robust, not driven by outliers.\n",
    "\n",
    "\n",
    "In both graphs, the long-short portfolio’s growth is consistent with our hypothesis: firms repeating language more heavily (higher similarity) tend to earn systematically higher returns compared to firms with more novel disclosures.\n",
    "\n",
    "\n",
    "Visual comparison against the S&P 500 line clearly shows that disclosure similarity contains real predictive power, aligning with the original findings of the Lazy Prices paper.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
